---
title: "Data Wrangling"
author: "Fabian Schroeder"
date: "April 9, 2018"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt = "true", strip.white=TRUE, comment = NA, cache = TRUE)
```

Data wrangling usually consists of several steps and includes: loading the data into the workspace, reformatting and restructuring the data. 

## Data Import

The first step in any data analysis is to load the data into your workspace. This seemingly easy task can be quite challenging since

- data come in different formats 
- data come in different sizes (and can exceed your memory)

The format of the data does not only include the kind of file the data is saved in but also how the data is encoded, etc. Most data sets come in tabular or spreadsheet-like form, which means that each row stands for an observation and each coloumn for a variable. This kind of data is often referred to as flat data and can come in different file formats, eg. .csv, .txt, .xlsx. R can also import data from other statistical software platforms such as STATA, SPSS, SAS.
Other forms include XML or HTML data, which is highly structured but not flat data.

An important criteria for the choice of method is the size of the data. If the data set exceeds the size of you memory you cannot load the entire data set into your workspace and will have to work with databases. 

This lesson is supposed to give a basic overview over the different kind of methods used to import data into the workspace. 


### `read.table()` and derivatives

The function `read.table`is the most convenient way to read in a rectangular grid of data. It belongs to the `utils`package and is loaded by default. Because of the many possibilities, there are several other function that call `read.table` but change a group of default arguments. These variants are `read.csv()`, `read.csv2()`, , `read.delim()`, `read.delim2()`. 

The number of options can be seen from its help page

```{r eval = FALSE} 
read.table(file, header = FALSE, sep = "", quote = "\"'",
           dec = ".", numerals = c("allow.loss", "warn.loss", "no.loss"),
           row.names, col.names, as.is = !stringsAsFactors,
           na.strings = "NA", colClasses = NA, nrows = -1,
           skip = 0, check.names = TRUE, fill = !blank.lines.skip,
           strip.white = FALSE, blank.lines.skip = TRUE,
           comment.char = "#",
           allowEscapes = FALSE, flush = FALSE,
           stringsAsFactors = default.stringsAsFactors(),
           fileEncoding = "", encoding = "unknown", text, skipNul = FALSE)

read.csv(file, header = TRUE, sep = ",", quote = "\"",
         dec = ".", fill = TRUE, comment.char = "", ...)

read.csv2(file, header = TRUE, sep = ";", quote = "\"",
          dec = ",", fill = TRUE, comment.char = "", ...)

read.delim(file, header = TRUE, sep = "\t", quote = "\"",
           dec = ".", fill = TRUE, comment.char = "", ...)

read.delim2(file, header = TRUE, sep = "\t", quote = "\"",
            dec = ",", fill = TRUE, comment.char = "", ...)

```

* `file`  The first argument is the name of the file which the data are to be read from. If it does not contain the absolute path, the file name is relative to the current working directory. Use `getwd()` to print it. `file` can also be a complete URL.

* `header` a logical value indicating whether the file contains the names of the variables as its first line. If missing, the value is determined from the file format: header is set to TRUE if and only if the first row contains one fewer field than the number of columns.

* `sep` the field separator character. Values on each line of the file are separated by this character. If sep = "" (the default for read.table) the separator is ‘white space’, that is one or more spaces, tabs, newlines or carriage returns.

* `dec` the character used in the file for decimal points.

* `na.strings`a character vector of strings which are to be interpreted as NA values. Blank fields are also considered to be missing values in logical, integer, numeric and complex fields.

* `skip` integer: the number of lines of the data file to skip before beginning to read data.

* `quote` the set of quoting characters. To disable quoting altogether, use quote = "". See scan for the behaviour on quotes embedded in quotes. Quoting is only considered for columns read as character, which is all of them unless colClasses is specified.

```{r }
library("datasets")
write.csv(iris, "iris.csv")
rm(list=ls())

iris <- read.csv("iris.csv")
str(iris)

```


### `readr` of the Tidyverse

The Tidyverse contains its own implementation of these functions, which are bundled in the package `readr`.

```{r eval = FALSE}
library(readr)
```

The functionality of the function `read_csv` and `read_tsv` corresponds grosso mode to the functions `read.csv` and `read.delim` of the `utils` package, howecer, it is

* faster
* the argument `stringsAsFactors` is set to FALSE by default
* output is a tibble, a special kind of data.frame

``` {r eval = FALSE}
read_delim(file, delim, quote = "\"", escape_backslash = FALSE,
  escape_double = TRUE, col_names = TRUE, col_types = NULL,
  locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
  comment = "", trim_ws = FALSE, skip = 0, n_max = Inf,
  guess_max = min(1000, n_max), progress = show_progress())

read_csv(file, col_names = TRUE, col_types = NULL,
  locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
  quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
  guess_max = min(1000, n_max), progress = show_progress())

read_csv2(file, col_names = TRUE, col_types = NULL,
  locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
  quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
  guess_max = min(1000, n_max), progress = show_progress())

read_tsv(file, col_names = TRUE, col_types = NULL,
  locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
  quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
  guess_max = min(1000, n_max), progress = show_progress())

```

```{r }
library(readr)
iris2 <- read_csv("iris.csv")
str(iris)

```

The functions in `readr` print more output to the console which help the user verify that the data has been correctly imported. Notice, that the Species column was not saved as a factor but remained a character string. In this regards the default settings differ.

### `data.table`

The `data.table` package developed by Matt Dowle and Arun Srinivasan has one objective: to increase the speed of data importation. It is

- super fast and
- produces objects of class `data.table`, a special kind of data.frame.


``` {r }
library(data.table)

iris3 <- fread("iris.csv")
str(iris3)
```

The output of this function is of class `data.table` a child of `data.frame`. As in the `readr` package `fread()` does not save character strings as factors by default. 
There is much more to the `data.table` package, which can be found in a separate data camp course. 


### `read.xlsx`

There are a few packages that import data from .xlsx files. My package of choice would be the `readxl` package by Hadley Wickham. `xlsx` and `XLConnect` also provide functions that can read from .xlsx files. However, they require `rJava` and are terribly slow.

``` {r eval = FALSE}
install.packages("readxl")
library(readxl)
```

The first handy function lists all sheets in an excel speadsheet. 

``` {r eval = FALSE}
excel_sheets(path)
```

The actual reading of the file is implemented in `read_excel`

```{r eval = FALSE}
read_excel(path, sheet = NULL, range = NULL, col_names = TRUE,
  col_types = NULL, na = "", trim_ws = TRUE, skip = 0, n_max = Inf,
  guess_max = min(1000, n_max))
```

* `path` Path to the xls/xlsx file
*  `sheet` Sheet to read. Either a string (the name of a sheet), or an integer (the position of the sheet). Ignored if the sheet is specified via range. If neither argument specifies the sheet, defaults to the first sheet.
* `range` A cell range to read from, as described in cell-specification. Includes typical Excel ranges like "B3:D87", possibly including the sheet name like "Budget!B2:G14", and more. Interpreted strictly, even if the range forces the inclusion of leading or trailing empty rows or columns. Takes precedence over skip, n_max and sheet.
* `col_names` TRUE to use the first row as column names, FALSE to get default names, or a character vector giving a name for each column. If user provides col_types as a vector, col_names can have one entry per column, i.e. have the same length as col_types, or one entry per unskipped column.
* `col_types` Either NULL to guess all from the spreadsheet or a character vector containing one entry per column from these options: "skip", "guess", "logical", "numeric", "date", "text" or "list". If exactly one col_type is specified, it will be recycled. The content of a cell in a skipped column is never read and that column will not appear in the data frame output. A list cell loads a column as a list of length 1 vectors, which are typed using the type guessing logic from col_types = NULL, but on a cell-by-cell basis.

* `na` Character vector of strings to use for missing values. By default, readxl treats blank cells as missing data.

* `skip`	Minimum number of rows to skip before reading anything, be it column names or data. Leading empty rows are automatically skipped, so this is a lower bound. Ignored if range is given.
* `n_max` Maximum number of data rows to read. Trailing empty rows are automatically skipped, so this is an upper bound on the number of rows in the returned tibble. Ignored if range is given.


The easiest way to import all spreadsheets from an excel file and save them into a list is by

```{r eval = FALSE}
my_workbook <- lapply(excel_sheets("data.xlsx"), read_excel, path = "data.xlsx")

```

Currently there is not possibility to write .xlsx files from the R console with this package. Other packages, however, provide this function. 

### Other formats

The `haven` package of the Tidyverse provides all function to import and export data from/for SPSS, Stata, and SAS. 

```{r eval = FALSE}
install.packages("haven")
library(haven)

```
The necessary functions are

```{r eval = FALSE}
read_sav(file, user_na = FALSE)

read_por(file, user_na = FALSE)

read_dta(file, encoding = NULL)

read_sas(data_file, catalog_file = NULL, encoding = NULL,
  catalog_encoding = encoding, cols_only = NULL)

```


### Import XML Data

When reading data from text files, it is the responsibility of the user to know and to specify the conventions used to create that file, e.g. the comment character, whether a header line is present, the value separator, the representation for missing values (and so on) described in Export to text files. A markup language which can be used to describe not only content but also the structure of the content can make a file self-describing, so that one need not provide these details to the software reading the data.

The eXtensible Markup Language – more commonly known simply as XML – can be used to provide such structure, not only for standard datasets but also more complex data structures. XML is becoming extremely popular and is emerging as a standard for general data markup and exchange. It is being used by different communities to describe geographical data such as maps, graphical displays, mathematics and so on. 
The function necessary to parse an XML document, `xmlInternatTreeParse()` or `xmlTreeParse()`, are in the `XML` package and can be installed via

``` {r eval = FALSE}
install.packages("XML")
library(XML)
```


### Importing from Databases

Instead of loading all the data into RAM, as R does, databases query data from the hard-disk. This can allow a subset of a very large dataset to be defined and read into R quickly, without having to load it first. However, this depends of the kind of data base that we want to load from. There are many different kinds of databases. An excellent overview over the differenct packages and functions can be found under
[https://db.rstudio.com/databases/](https://db.rstudio.com/databases/).

As an example, let us have a look at the `RODBC` package. It is one the most mature packages of this kind and sets up links to external databases using the Open Database Connectivity (ODBC) API. `RODBC` connects to ‘traditional’ databases such as MySQL, PostgreSQL, Oracle and SQLite.

The function used to set-up a connection to an external database with `RODBC` is `odbcConnect`, which takes Data Source Name (`dsn`), User ID (`uid`) and password (`pwd`) as required arguments. 

```{r eval=FALSE}
require(RODBC)

#open the ODBC connection
ch <- odbcConnect("ODBCDriverName")

##### Alternative ODBC connection for Microsoft SQL Server
ch <- odbcDriverConnect(
                        "Driver=SQL Server; Server=servername\\instance; Database=databasename; UID=username; Pwd=password"
                       )

#run the query, store in a data frame
sqlResult <- sqlQuery(ch, "SELECT ...
                FROM ...
                WHERE ...
                ;")

#close the ODBC connection
odbcClose(ch)

```


A new development is the ability to interact with databases using exactly the same syntax used to interact with R objects stored in RAM. This innovation was made possible by dplyr, an R library for data processing that aims to provide a unified ‘front end’ to perform a wide range of analysis task on datasets using a variety of ‘back ends’ which do the number crunching. This is one of the main advantages of `dplyr`. It translates the data wrangling function from the `dplyr` package to `SQL` queries. Thus, your R code is translated into SQL and executed in the database, not in R. When working with databases, dplyr tries to be as lazy as possible:

* It never pulls data into R unless you explicitly ask for it.
* It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.



## Data tidying with `tidyr`

The contents of this lesson are based on the 2014 paper "Tidy Data" by Hadley Wickham from the Journal of Statistical Software. 

It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003). This process is often referred to as
data wrangling. It includes many steps, such as outlier checking and data imputation, reformating variables especially dates, etc. The `tidyr` package provides functions for one of these tasks: tidying: structuring datasets to facilitate analysis. 

```{r eval = TRUE}
library(tidyr)
```

The principles of tidy data provide a standard way to organize data values within a dataset.
A standard makes initial data cleaning easier because you do not need to start from scratch
and reinvent the wheel every time. The tidy data standard has been designed to facilitate
initial exploration and analysis of the data, and to simplify the development of data analysis
tools that work well together.

The paper provides a set of principles of how to structure data, that is the fundamental aproach 
for all packages in the tidyverse. We will use the `dplyr` and the `ggplot2` package which share
these common ideas.

### Tidy data

Let us first define a few terms:

A dataset is a collection of *values*, usually either numbers (if quantitative) or strings (if
qualitative). Values are organized in two ways. Every value belongs to a *variable* and an
*observation*. A variable contains all values that measure the same underlying attribute (like
height, temperature, duration) across units. An observation contains all values measured on
the same unit (like a person, or a day, or a race) across attributes.

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr1.png){width=70%}</center>

Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is
messy or tidy depending on how rows, columns and tables are matched up with observations,
variables and types. In tidy data:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr2.png){width=50%}</center>

This table is the tidy version of the first one. Each row represents an observation, the result of one
treatment on one person, and each column is a variable.

For a given dataset, it is usually easy to figure out what are observations and what are vari-
ables, but it is surprisingly difficult to precisely define variables and observations in general.
For example, if the columns in the Table 1 were height and weight we would have been happy
to call them variables. If the columns were height and width, it would be less clear cut, as
we might think of height and width as values of a dimension variable.

Tidy data makes it easy for an analyst or a computer to extract needed variables because it
provides a standard way of structuring a dataset. Compare Table 3 to Table 1: in Table 1
you need to use different strategies to extract different variables. This slows analysis and
invites errors. If you consider how many data analysis operations involve all of the values in a
variable (every aggregation function), you can see how important it is to extract these values
in a simple, standard way. Tidy data is particularly well suited for vectorized programming
languages like R (R Core Team 2014), because the layout ensures that values of different
variables from the same observation are always paired.

### Untidy data

In order to understand the structure of tidy data better it makes sense to look at examples of untidy data.

According to Hadley Wickham the five most common problem with untidy datasets are:

* Column headers are values, not variable names.
* Multiple variables are stored in one column.
* Variables are stored in both rows and columns.
* Multiple types of observational units are stored in the same table.
* A single observational unit is stored in multiple tables.

Let us have a look at untidy data sets taken from the book R for Data Science.

```
table1

#> # A tibble: 6 × 4
#>       country  year  cases population
#>         <chr> <int>  <int>      <int>
#> 1 Afghanistan  1999    745   19987071
#> 2 Afghanistan  2000   2666   20595360
#> 3      Brazil  1999  37737  172006362
#> 4      Brazil  2000  80488  174504898
#> 5       China  1999 212258 1272915272
#> 6       China  2000 213766 1280428583

table2

#> # A tibble: 12 × 4
#>       country  year       type     count
#>         <chr> <int>      <chr>     <int>
#> 1 Afghanistan  1999      cases       745
#> 2 Afghanistan  1999 population  19987071
#> 3 Afghanistan  2000      cases      2666
#> 4 Afghanistan  2000 population  20595360
#> 5      Brazil  1999      cases     37737
#> 6      Brazil  1999 population 172006362
#> # ... with 6 more rows

table3

#> # A tibble: 6 × 3
#>       country  year              rate
#> *       <chr> <int>             <chr>
#> 1 Afghanistan  1999      745/19987071
#> 2 Afghanistan  2000     2666/20595360
#> 3      Brazil  1999   37737/172006362
#> 4      Brazil  2000   80488/174504898
#> 5       China  1999 212258/1272915272
#> 6       China  2000 213766/1280428583

# Spread across two tibbles

table4a  # cases

#> # A tibble: 3 × 3
#>       country `1999` `2000`
#> *       <chr>  <int>  <int>
#> 1 Afghanistan    745   2666
#> 2      Brazil  37737  80488
#> 3       China 212258 213766

table4b  # population

#> # A tibble: 3 × 3
#>       country     `1999`     `2000`
#> *       <chr>      <int>      <int>
#> 1 Afghanistan   19987071   20595360
#> 2      Brazil  172006362  174504898
#> 3       China 1272915272 1280428583
```
These are all representations of the same underlying data, but they are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse.

### Tidying data

The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:

* One variable might be spread across multiple columns.
* One observation might be scattered across multiple rows.

For these purposes there are two functions in tidyr: `gather()` and `spread()`. 

* Two variables might be recorded in one column
* One variable might be recorded in two columns

For these purposes there are two functions in tidyr: `separate()` and `unite()`


### `gather()' 

Gather takes multiple columns and collapses into key-value pairs, duplicating all other columns as needed. You use gather() when you notice that you have columns that are not variables.

#### Usage

```
gather(data, key = "key", value = "value", ..., na.rm = FALSE,
  convert = FALSE, factor_key = FALSE)
```

In order to understand this function one must understand the following three arguments:

`data`  A data frame.

`key`, `value`  Names of new key and value columns, as strings or symbols.  

`...` A selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between x and z with x:z, exclude y with -y. 


Let us have a look at an example for a data set where some of the column names are not names of variables, but values of a variable (taken Example from R for Data Science)

```
table4a

#> # A tibble: 3 × 3
#>       country `1999` `2000`
#> *       <chr>  <int>  <int>
#> 1 Afghanistan    745   2666
#> 2      Brazil  37737  80488
#> 3       China 212258 213766
```
We can use gather to make the data tidy

```
table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")
  
#> # A tibble: 6 × 3
#>       country  year  cases
#>         <chr> <chr>  <int>
#> 1 Afghanistan  1999    745
#> 2      Brazil  1999  37737
#> 3       China  1999 212258
#> 4 Afghanistan  2000   2666
#> 5      Brazil  2000  80488
#> 6       China  2000 213766
```

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr3.png)</center>

> Excercise

The layout of the following data frame stock may be accessible to the human eye, however it is not tidy according 
to the rules laid out earlier.

```{r}
library(dplyr)
# From http://stackoverflow.com/questions/1181060
stocks <- tibble(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)

```

In this example X, Y, and Z are not different variables but values of a variable called stock. How can use separate to tidy the data?

```{r }
gather(stocks, stock, price, -time)
stocks %>% gather(stock, price, -time) # the same command using the pipe operator
```

### `spread()`

Spreading is the opposite of gathering. You use it when an observation is scattered across multiple rows. 

#### Usage

```
spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE,
  sep = NULL)
```
The most important arguments are 
`data` a data frame

`key`, `value` Column names or positions.

Again, let us have a look at an example from the book R for Data Science.

```
table2
#> # A tibble: 12 × 4
#>       country  year       type     count
#>         <chr> <int>      <chr>     <int>
#> 1 Afghanistan  1999      cases       745
#> 2 Afghanistan  1999 population  19987071
#> 3 Afghanistan  2000      cases      2666
#> 4 Afghanistan  2000 population  20595360
#> 5      Brazil  1999      cases     37737
#> 6      Brazil  1999 population 172006362
#> # ... with 6 more rows

```

An observation is a country in a year, but each observation is spread across two rows.

```
spread(table2, key = type, value = count)
#> # A tibble: 6 × 4
#>       country  year  cases population
#> *       <chr> <int>  <int>      <int>
#> 1 Afghanistan  1999    745   19987071
#> 2 Afghanistan  2000   2666   20595360
#> 3      Brazil  1999  37737  172006362
#> 4      Brazil  2000  80488  174504898
#> 5       China  1999 212258 1272915272
#> 6       China  2000 213766 1280428583
```

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr4.png)</center>

> Excercise

Consider the gathered stocks data from last excercise. 

```{r }
stocksm <- stocks %>% gather(stock, price, -time)
```

In our analysis we want to consider a stock an observation and the price at a given point in time is considered a variable. How can we use the `spread()` function, to create the proper formatting.

```{r }
stocksm %>% spread(time, price)
# another possibility would be to spread with key = stock
stocksm %>% spread(stock, price)
```


> Excercise C

Are `gather()` and `spread()` complementary operations?

```{r }
df1 <- data.frame(x = c("a", "b"), y = c(3, 4), z = c(5, 6))
df2 <- df1 %>% spread(x, y) %>% gather(x, y, a:b, na.rm = TRUE)
df1==df2[,c(2,3,1)]
```


### `separate()`

This function pull apart one column into multiple columns by spliiting wherever a separator character appears.

#### Usage

```
separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE,
  convert = FALSE, extra = "warn", fill = "warn", ...)
```
The most important arguments are
`data` the data frame

`col` the column name or position

`into` the names of the new variables to create as character vector

`sep` the separator between columns.


```
table3
#> # A tibble: 6 × 3
#>       country  year              rate
#> *       <chr> <int>             <chr>
#> 1 Afghanistan  1999      745/19987071
#> 2 Afghanistan  2000     2666/20595360
#> 3      Brazil  1999   37737/172006362
#> 4      Brazil  2000   80488/174504898
#> 5       China  1999 212258/1272915272
#> 6       China  2000 213766/1280428583
```

The rate contains both `cases` and `population` variables. We, thus, need to split it into two variables.

```
table3 %>% 
  separate(rate, into = c("cases", "population"))
#> # A tibble: 6 × 4
#>       country  year  cases population
#> *       <chr> <int>  <chr>      <chr>
#> 1 Afghanistan  1999    745   19987071
#> 2 Afghanistan  2000   2666   20595360
#> 3      Brazil  1999  37737  172006362
#> 4      Brazil  2000  80488  174504898
#> 5       China  1999 212258 1272915272
#> 6       China  2000 213766 1280428583

```

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr5.png)</center>


### `unite()`

This is the inverse of `separate()`. It combines multiple columns into a single column.

#### Usage

```
separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE,
  convert = FALSE, extra = "warn", fill = "warn", ...)
```

Its most important arguments are

`data` the data frame

`col` the name of the new column, as a string or symbol.

`...` A selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between x and z with x:z, exclude y with -y.

`sep` Separator to use between values.

```

table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
#> # A tibble: 6 × 4
#>       country century  year              rate
#> *       <chr>   <chr> <chr>             <chr>
#> 1 Afghanistan      19    99      745/19987071
#> 2 Afghanistan      20    00     2666/20595360
#> 3      Brazil      19    99   37737/172006362
#> 4      Brazil      20    00   80488/174504898
#> 5       China      19    99 212258/1272915272
#> 6       China      20    00 213766/1280428583

```
The century and year column can be united with the `unite()` function.

```
table5 %>% 
  unite(new, century, year)
#> # A tibble: 6 × 3
#>       country   new              rate
#> *       <chr> <chr>             <chr>
#> 1 Afghanistan 19_99      745/19987071
#> 2 Afghanistan 20_00     2666/20595360
#> 3      Brazil 19_99   37737/172006362
#> 4      Brazil 20_00   80488/174504898
#> 5       China 19_99 212258/1272915272
#> 6       China 20_00 213766/1280428583
```

<center>![](/home/Fabian2/Desktop/StatComp/figures/tidyr6.png)</center>



### Data transformation with `dplyr`

This lesson is a selection of the 5th chapter of the book "R for Data Science" by Hadley Wickham.

The `dplyr` package contain a number of function that help in transforming the data in a certain way 

* Pick observations by their values (`filter()`).
* Reorder the rows (`arrange()`).
* Pick variables by their names (`select()`).
* Create new variables with functions of existing variables (`mutate()`).
* Collapse many values down to a single summary (`summarise()`).

These can all be used in conjunction with `group_by()` which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.

All verbs work similarly:

* The first argument is a data frame.
* The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).
* The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. 

```{r echo = FALSE, warning=FALSE}
library(tidyverse)
```


### Data: nycflights13

To explore the basic data manipulation verbs of dplyr, we’ll use nycflights13::flights. This data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented in `?flights`.

```{r }
library(nycflights13)
flights
```

### Filter rows with *filter()* 

`filter()` allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all flights on January 1st with:

```{r }
filter(flights, month == 1, day == 1)

```


Multiple conditions are combined with &.


Exercise 1. Find all flights that

  a. Had an arrival delay of two or more hours
  b. Flew to Houston (IAH or HOU)
  c. Were operated by United, American, or Delta
  d. Departed in summer (July, August, and September)
  e. Arrived more than two hours late, but didn’t leave late
  f. Were delayed by at least an hour, but made up over 30 minutes in flight
  g. Departed between midnight and 6am (inclusive)

```{r eval = FALSE}
# 1a.
filter(flights, arr_delay > 120)

# 1b. 
filter(flights, dest=="IAH" | dest=="HOU")

# 1c.
filter(flights, carrier %in% c("UA", "AA", "DL"))

# 1d.
filter(flights, month >= 6, month <= 8)

# 1e.
filter(flights, arr_delay > 120 & dep_delay > 0)

# 1f.
filter(flights, dep_delay > 60 & arr_delay < dep_delay - 30)

# 1g.
filter(flights, dep_time < 6)
```



Exercise 2. Another useful dplyr filtering helper is `between()`. What does it do? Can you use it to simplify the code needed to answer the previous challenges?

```{r eval = FALSE}
filter(flights, between(month, 6, 8))
```

Exercise 3. How many flights have a missing dep_time? What other variables are missing? What might these rows represent?
```{r eval=FALSE}
filter(flights, is.na(dep_time))
```


### Arrange rows with `arrange()`

`arrange()` works similarly to `filter()` except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r }
arrange(flights, year, month, day)
```
Use `desc()` to re-order by a column in descending order:
```{r }
arrange(flights, desc(arr_delay))
```

Missing values are always sorted at the end:

```{r collapse=TRUE}
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
arrange(df, desc(x))
```


Exercise 1. How could you use `arrange()` to sort all missing values to the start? (Hint: use `is.na()`).

```{r eval = FALSE}
arrange(flights, !is.na(dep_time))
```

Exercise 2. Sort flights to find the most delayed flights. Find the flights that left earliest.

```{r eval = FALSE}
arrange(flights, desc(arr_delay))
arrange(flights, dep_delay)
```

Exercise 3. Sort flights to find the fastest flights.

```{r eval = FALSE}
arrange(flights, air_time/distance)
```

Exercise 4. Which flights travelled the longest? Which travelled the shortest?

```{r eval = FALSE}
arrange(flights, desc(air_time))
arrange(flights, air_time)
```

### Select columns with `select()`

It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.

```{r collapse = TRUE}
select(flights, year, month, day)
select(flights, year:day)
select(flights, -(year:day))

```

There are a number of helper functions you can use within `select()`:

* `starts_with("abc")`: matches names that begin with “abc”.
* `ends_with("xyz")`: matches names that end with “xyz”.
* `contains("ijk")`: matches names that contain “ijk”.
* `matches("(.)\\1")`: selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings.
* `num_range("x", 1:3)` matches x1, x2 and x3.

See `?select` for more details.

`select()` can be used to rename variables, but it’s rarely useful because it drops all of the variables not explicitly mentioned. Instead, use `rename()`, which is a variant of `select()` that keeps all the variables that aren’t explicitly mentioned:

Another option is to use `select()` in conjunction with the `everything()` helper. This is useful if you have a handful of variables you’d like to move to the start of the data frame.

```{r }
select(flights, time_hour, air_time, everything())
```

Exercise 1. Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.

```{r eval=FALSE}
flights[,c(4,6,7,9)]
select(flights, dep_time, dep_delay, arr_time, arr_delay)
select(flights, starts_with("dep"), starts_with("arr"))
```

Exercise 2. What happens if you include the name of a variable multiple times in a `select()` call?

```{r }
select(flights, dep_time, dep_time)
```


Exercise 3. What does the `one_of()` function do? Why might it be helpful in conjunction with this vector?
```
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
```
```{r }
vars <- c("year", "month", "day", "dep_delay", "arr_delay") 
select(flights, one_of(vars))
```


Exercise 4. Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?

``` 
select(flights, contains("TIME"))
```
```{r eval=FALSE}
select(flights, contains("TIME"))
select(flights, contains("TIME", ignore.case=FALSE))
```


### Add new variables with `mutate()`

Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of `mutate().`

```{r }
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)

mutate(flights_sml,
  gain = arr_delay - dep_delay,
  speed = distance / air_time * 60
)
```

Note that you can refer to columns that you’ve just created:

```{r }
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

If you only want to keep the new variables, use `transmute()`:

```{r }
transmute(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

There are many functions for creating new variables that you can use with `mutate()`. The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output.


Exercise 1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.

```{r }
select(flights, dep_time, sched_dep_time)
flights_ext <- mutate(flights, 
                      dep_time_min = floor(dep_time / 100)*60 + dep_time %% 100,
                      sched_dep_time_min = floor(sched_dep_time / 100)*60 + sched_dep_time %% 100)
select(flights_ext, dep_time, sched_dep_time, dep_time_min, sched_dep_time_min)
```

Exercise 2. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?

```{r }
select(flights, air_time, arr_time, dep_time)
flights_ext <- mutate(flights_ext, arr_time_min = floor(arr_time / 100)*60 + arr_time %% 100,
                               air_time_min = arr_time_min - dep_time_min)
select(flights_ext, arr_time_min, dep_time_min, air_time_min)
```

Exercise 3. Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?
```{r collapse=T}
select(flights, dep_time, sched_dep_time, dep_delay)
table(flights$sched_dep_time + flights$dep_delay == flights$dep_time)
table(flights_ext$dep_time_min - flights_ext$sched_dep_time_min == flights_ext$dep_delay)
# does not fix delays over the time point 00:00
```


### Grouped summaries with `summarise()`

The last key verb is `summarise()`. It collapses a data frame to a single row:

```{r }
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

`summarise()` is not terribly useful unless we pair it with `group_by()`. This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date:

```{r }
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

Together `group_by()` and `summarise()` provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries.


### Combining multiple operations with the pipe

The elegance of these transformation functions manifest when used in combination with the pipe operator:

```{r }
flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL") %>%
  ggplot(mapping = aes(x = dist, y = delay)) +
  geom_point(aes(size = count), alpha = 1/3) +
  geom_smooth(se = FALSE)
```



### Useful summary functions

- Measures of location: `mean()` and `median()`

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(year, month, day) %>% 
  summarise(
    avg_delay1 = mean(arr_delay),
    avg_delay2 = mean(arr_delay[arr_delay > 0]) # the average positive delay
  )
```

- Measure of spread: `sd(x)`, `IQR(x)`, `mad(x)`. The mean squared deviation, or standard deviation or sd for short, is the standard measure of spread. The interquartile range `IQR()` and median absolute deviation `mad(x)` are robust equivalents that may be more useful if you have outliers.

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(dest) %>% 
  summarise(distance_sd = sd(distance),
            distance_iqr = IQR(distance),
            distance_mad = mad(distance)) %>% 
  arrange(desc(distance_sd))
```

- Measures of position: `first(x)`, `nth(x, 2)`, `last(x)`. For example, we can find the first and last departure for each day:

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(year, month, day) %>% 
  summarise(
    first_dep = first(dep_time), 
    last_dep = last(dep_time)
  )
```

- counts: `n()` takes no arguments, and returns the size of the current group. To count the number of non-missing values, use `sum(!is.na(x))`. To count the number of distinct (unique) values, use `n_distinct(x)`.

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(dest) %>% 
  summarise(carriers = n_distinct(carrier)) %>% 
  arrange(desc(carriers))
```

When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

```{r eval = FALSE}
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
(per_year  <- summarise(per_month, flights = sum(flights)))
```

Exercises 1. Which carrier has the worst delays?

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(carrier) %>% 
  summarise(count = n(), 
            avg_delay = mean(arr_delay, rm.na=TRUE)) %>%
  arrange(avg_delay, count) 
```

Exercise 2. Which flight is always 10 minutes late?

```{r }
flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(flight) %>%
  summarize(min_delay = min(arr_delay),
            median_delay = median(arr_delay),
            count = n()) %>%
  arrange(desc(min_delay)) %>%
  filter(min_delay > 10, count > 3)

```









